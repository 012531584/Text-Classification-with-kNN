{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the text data and do the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import operator \n",
    "\n",
    "data_df = pd.read_csv('train.dat',header=None,names=['Class', 'Text'],sep='\\t')\n",
    "test_df = pd.read_csv('test.dat',header=None,names=['Text'],sep='\\t')\n",
    "\n",
    "\n",
    "# convert to lowercase\n",
    "data_df.Text= data_df.Text.apply(lambda x: x.lower())\n",
    "test_df.Text= test_df.Text.apply(lambda x: x.lower()) \n",
    "\n",
    "#remove punctuation\n",
    "data_df.Text= data_df.Text.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "test_df.Text= test_df.Text.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "\n",
    "#remove digit\n",
    "data_df.Text= data_df.Text.apply(lambda x: x.translate(str.maketrans('','',string.digits)))\n",
    "test_df.Text= test_df.Text.apply(lambda x: x.translate(str.maketrans('','',string.digits)))\n",
    "\n",
    "#remove stop words    (required to download 'stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "data_df.Text= data_df.Text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "test_df.Text= test_df.Text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# remove common words\n",
    "freq_common1 = pd.Series(' '.join(data_df.Text).split()).value_counts()[:10]    \n",
    "#print(freq)\n",
    "freq_common1 = list(freq_common1.index)\n",
    "data_df.Text= data_df.Text.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_common1))\n",
    "\n",
    "freq_common2 = pd.Series(' '.join(test_df.Text).split()).value_counts()[:10]    \n",
    "#print(freq)\n",
    "freq_common2 = list(freq_common2.index)\n",
    "test_df.Text= test_df.Text.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_common2))\n",
    "\n",
    "\n",
    "# remove rare words\n",
    "freq_rare1 = pd.Series(' '.join(data_df.Text).split()).value_counts()[-10:]    \n",
    "#print(freq_rare)\n",
    "freq_rare1 = list(freq_rare1.index)\n",
    "data_df.Text= data_df.Text.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_rare1))\n",
    "\n",
    "freq_rare2 = pd.Series(' '.join(test_df.Text).split()).value_counts()[-10:]    \n",
    "#print(freq_rare)\n",
    "freq_rare2 = list(freq_rare2.index)\n",
    "test_df.Text= test_df.Text.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_rare2))\n",
    "\n",
    "# remove suffices (reduce a lot ncols)\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "data_df.Text= data_df.Text.apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "test_df.Text= test_df.Text.apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "# Lemmatization\n",
    "from textblob import Word\n",
    "data_df.Text= data_df.Text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "test_df.Text= test_df.Text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "docs = [t.split() for t in data_df.Text]\n",
    "docs_test = [t.split() for t in test_df.Text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit the word's length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLen(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if len(t) >= minlen ] for d in docs ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the text data to the C.S.R metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "def build_matrix(docs,ID):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = ID\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))  #unrepeated words in each row, and aggregated them\n",
    "        for w in d:\n",
    "            if w not in idx:    #key:words, values:ID; similar to plotWf\n",
    "                idx[w] = tid    #count how many unrepeated words\n",
    "                tid += 1\n",
    "    ncols = len(idx)        #unrepeated words in whole dataset\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)    # an array full of zero, has 'nnz' amount of element\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)    # store memory, with length  ' amount of object +1'\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)    #sorted dict. :  key=words, values=times\n",
    "        keys = list(k for k,_ in cnt.most_common())    # list in frequency order (most to least)\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):    # j for index, k for content\n",
    "            ind[j+n] = idx[k]    # pass the words' ID value\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l    #ptr[0] don't touch, ptr[i+1] value is amount of set(words) in each row add the previous ptr[i]\n",
    "        n += l    # Amounts of non-repeated words\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)    #(data,indices,indptr)->(frequency, ID, memory)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return (mat,idx)\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dataset with L2-norm method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale matrix and normalize its rows\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1    #count how many times appear in whole document, key=ID, value=Frequnecy\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]    # larger = more important\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def getResult(trainingSet, testInstance, k, epsilon, trainWithClass):\n",
    "    \n",
    "    result_set=[]\n",
    "    sim = cosine_similarity(trainingSet,testInstance)\n",
    "    \n",
    "    #test number ->0 to end \n",
    "    # ith test \n",
    "    for i in range(sim.shape[1]):\n",
    "        # get k neighbors\n",
    "        neighbors = []\n",
    "        for x in range(k):\n",
    "            index = sim[:,i].argsort()[-x-1]    #ith column of sim[][], get the sorting sequnence in index form\n",
    "            \n",
    "            if(sim[:,i][index]>epsilon):    #compare similarity value\n",
    "                neighbors.append(index)   \n",
    "            if(neighboers==0):\n",
    "                print(\"epsilon value too high\")\n",
    "                return False\n",
    "        result = getResponse(neighbors,trainWithClass)\n",
    "        result_set.append(result)\n",
    "        \n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(neighbors,train):\n",
    "    classVotes = {}    #majority vote\n",
    "    #print(\"neighbor len:\",len(neighbors))\n",
    "    for x in range(len(neighbors)):\n",
    "        response = train.Class[neighbors[x]]    #last element\n",
    "        #print(\"response:\", response)\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "        \n",
    "        \n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply functions to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter words length\n",
    "docs1 = filterLen(docs, 4)  \n",
    "docs_test1 = filterLen(docs_test, 4)\n",
    "docs_sum = docs1+docs_test1    # to get whole words ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 14438, ncols 156433, nnz 1245863]\n",
      " [nrows 14442, ncols 156433, nnz 1270791]\n"
     ]
    }
   ],
   "source": [
    "#transfer matrix\n",
    "total_ID={}\n",
    "(mat1, total_ID) = build_matrix(docs_sum,total_ID)\n",
    "(train_mat1,total_ID) = build_matrix(docs1,total_ID)\n",
    "(test_mat1,total_ID) = build_matrix(docs_test1,total_ID)\n",
    "csr_info(train_mat1)\n",
    "csr_info(test_mat1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat2 = csr_idf(train_mat1, copy=True)\n",
    "test_mat2 = csr_idf(test_mat1, copy=True)\n",
    "train_mat3 = csr_l2normalize(train_mat2, copy=True)\n",
    "test_mat3 = csr_l2normalize(test_mat2, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 14438, ncols 46379, nnz 866351]\n",
      " [nrows 14442, ncols 46379, nnz 881093]\n",
      "14442\n",
      "14438\n"
     ]
    }
   ],
   "source": [
    "csr_info(train_mat3)\n",
    "csr_info(test_mat3)\n",
    "print(test_mat3.shape[0])\n",
    "print(train_mat3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the prediction of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 126.15117001533508\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "k=5\n",
    "epsilon=0.02\n",
    "ans = getResult(train_mat3, test_mat3, k, epsilon, data_df)    #(train data; test data; k; train data with class)\n",
    "\n",
    "\n",
    "#print(result)\n",
    "end=time.time()\n",
    "print('time:',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output label file\n",
    "with open('prediction.dat', mode='w') as out:\n",
    "    writer = csv.writer(out)\n",
    "    for i in range(len(ans)):\n",
    "        writer.writerow([ans[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
